---
layout: post
title:  "4. 다양한 분류 알고리즘"
date:   2024-08-13 01:26:30 +09:00
categories: ['ML기초세션']
---


## 1. 로지스틱 회귀
어떤 샘플이 어느 클래스에 속할지를 분류하는 문제를 우리는 해결했었다. 그렇다면 단순히 분류하는 것이 아니라, 각 클래스에 속할 **확률**을 구하는 것은 어떨까?

다음과 같은 DataFrame을 가지고 활용해보자.

![alt text](/assets/images/4_image.png)
```python
fish_input=fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target=fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

위와 같이 train-test 분리와 스케일링을 진행한다.

```python
print(kn.classes_)
#['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
print(kn.predict(test_scaled[:5]))
#['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
```
KNeighborsClassifier에서 정렬된 타깃값은 classes_에 저장되며, 위는 임의의 데이터에 대해 예측을 한 것이다.    
이때 predict_proba()를 통해 각 클래스에 속할 확률을 확인할 수 있다.

![alt text](/assets/images/4_image-1.png)

이때, 위의 과정을 통해 구한 Probability는 단순히 전체 이웃 중 타겟인 이웃의 개수를 확률로 나타낸 것에 지나지 않는다. 그렇다면 어떻게 더 유의미한 값을 도출할 수 있을까?

### 1-1. 로지스틱 

**로지스틱 회귀**는 **분류 모델**이다. 하지만 원리에서 **회귀**를 사용한다.   
이게 무슨 소리냐면, 이 모델은 기본적으로 선형 회귀의 아이디어에서 출발한다. 입력 변수들의 선형 결합을 계산한 후, 로지스틱 함수를 통해 0과 1 사이의 확률로 변환해 해당 확률을 기반으로 클래스 분류를 실시한다.


이게 무슨 말이냐면,   
![alt text](/assets/images/4_image-2.png) ![alt text](/assets/images/4_image-3.png)

이런 선형 결합 z를 로지스틱 함수(or 시그모이드 함수)를 통해 0과 1사이 값으로 변환한다는 것이다.   
0과 1사이의 값이니 여기서 확률적 의미를 찾을 수 있는 것이다!

이진 분류에서 0.5보다 크면 양성, 0.5보다 작으면 음성 클래스로 판단한다.

 ```python
 bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
# | 는 OR 연산자 이다. 
```

불리언 인덱싱을 통해 도미(Bream)과 빙어(Smelt)에 해당하는 행만 골라내자.

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
print(lr.predict(train_bream_smelt[:5]))
#['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
print(lr.predict_proba(train_bream_smelt[:5]))
#[[0.99759855 0.00240145]
# [0.02735183 0.97264817]
# [0.99486072 0.00513928]
# [0.98584202 0.01415798]
# [0.99767269 0.00232731]]
```
5개의 상위 로우에 대해 위와 같이 sklearn의 LogisticRegression()을 적용해 분류와 확률을 확인 할 수 있다.   
여기서 주목할 점은 Probability의 첫번째는 음성, 두번째는 양성 확률이라는 것이다.
knn과 동일하게 사이킷런은 알파벳순으로 class 타겟값을 정렬한다. 따라서 빙어(Smelt)가 양성이다.

아까 말했듯이 로지스틱 회귀의 기본 원리는 회귀이다. 따라서 엄연한 기울기와 절편이 존재한다!
```python
print(lr.coef_, lr.intercept_)
#[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
```
그리고 선형 결합의 결과인 z값도 각각 확인이 가능하다.
```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
#[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
```

### 1-2. 로지스틱 회귀와 다중 분류

다중 분류를 하기 전에, 로지스틱 회귀에 대한 몇 가지 사실을 정리하자   
- 반복적인 알고리즘을 사용하며, max_iter를 통해 조정할 수 있다.
- 릿지 회귀와 같이 **계수의 제곱을 규제**가 가능하다.(L2규제)
- 매개변수 C를 통해 규제의 양을 조절하는데, C가 작을수록 규제가 커진다. 

다중 분류를 적용하면 아래와 같다.
```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
#[[0.    0.014 0.841 0.    0.136 0.007 0.003]
# [0.    0.003 0.044 0.    0.007 0.946 0.   ]
# [0.    0.    0.034 0.935 0.015 0.016 0.   ]
# [0.011 0.034 0.306 0.007 0.567 0.    0.076]
# [0.    0.    0.904 0.002 0.089 0.002 0.001]]
```
확률이 7개의 클래스에 대해 제시되는 것을 확인할 수 있다.   
그런데 중요한 점이 있다. 다중 분류는 **클래스마다** z값을 계산한다.   
그리고 이후에 7개의 z값을 로지스틱 함수가 아닌 소프트맥스 함수를 통해 확률로 바꾼다.

왜냐하면 함수를 통과한 값들의 합이 1이 나와야 **확률**로서의 의미가 생기기 때문이다.

>**SoftMax Function**   
시그모이드 함수는 하나의 선형 방정식의 출력값을 0~1사이로 압축한다.    
이와 달리 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0~1로 압축해 합이 1이 되도록 만든다.

![alt text](/assets/images/4_image-4.png)
![alt text](/assets/images/4_image-5.png)

이렇게 만든다면 s1부터 s7까지의 합은 1이 되고, 각각은 확률의 의미를 지니지 않겠는가?

## 2. 확률적 경사 하강법

### 2-1. 확률적 경사 하강법이란
확률적 경사 하강법은 딥러닝에서 많이 쓰는 개념이다.
점진적으로, 기울기와 경향성을 따라 탐색을 하는 방법이다.

![alt text](/assets/images/4_image-6.png)

간략하게 용어들을 짚고 가도록 하겠다.
- 에포크: 훈련 데이터 전체를 모델이 한 번 학습하는 과정
- 미니 배치: 데이터셋을 여러 개의 작은 덩어리로 나눈 것
- 훈련데이터 1000개를 10개의 미니배치로 나눈다면 각 미니배치의 크기는 100이다.

이때 무작위로 몇개의 샘플을 골라서 미니배치 형태로 만든 다음 경사 하강법을 실행하는 것을 **미니배치 경사 하강법**이라고 한다. 이때 원래 SGD와 다르게 하나가 아닌 여러개의 미니배치 안의 샘플들이 한번에 배치단위로 실행된다.

이에 대한 자세한 내용은 '밑바닥부터 시작하는 딥러닝 1'을 참고하자.

### 2-2. 손실 함수

**손실 함수**는 Loss Function이라고 한다. 결국 무언가 얼마나 잘못되었느냐를 측정하는 함수이다.    
당연히 작게 나올수록 좋다. 그리고 이 손실 함수에는 다양한 종류가 있다.

로지스틱 손실 함수의 경우에는 양성 클래스일 경우 손실은 -log(probability), 음성 클래스일경우 -log(1-probability)를 사용한다. 예측 확률이 0에서 멀어질 수록 손실은 아주 큰 양수가 된다. 다르게 로그 로스라고도 한다.

로지스틱 손실 함수는 다시 말해 **이진 크로스엔트로피 손실 함수**이다. 다중 분류에서 사용되는 손실 함수는 그렇기에 그냥 **크로스엔트로피 손실 함수**이다.

그리고 회귀에서는 **평균 제곱 오차**라는 손실 함수를 사용한다.

그렇다면 손실 함수를 가장 작게 만드는 지점을 찾는다면 문제가 해결되지 않을까?

### 2-3. SGDClassifier

```python
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.773109243697479
#0.775
```
위는 max_iter, 즉 에폭을 10번으로 설정한 SGDClassifier이다. 꽤 낮은 정확도가 나온다.   
여기에 점진적 학습을 시켜보도록 하자. partial_fit()을 이용하면 1에폭씩 이어서 학습이 가능하다.
```python
sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.8151260504201681
#0.85
```
이건 결과적으로 11에폭 만큼의 학습을 진행한 것이다. 개선된 정확도를 확인할 수 있다.
> 직관적으로 이해할 수 있는 문제인데, 에폭수가 많을 수록 오버피팅될 확률이 높다

![alt text](/assets/images/4_image-7.png)

이는 300에폭 만큼 점진적으로 학습을 수행한 결과이다. 100번째 에폭 train-test 사이의 격차가 늘어나고 더이상 개선이 이루어 지지않는 것으로 보아, 100에폭이 적절한 반복 횟수이다.
```python
sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.957983193277311
#0.925
```
그리고 tol이라는 파라미터에 대해 주목하자. tol은 일정기간 동안 향상이 이루어지지 않으면 학습을 조기종료시키는 매개변수이다. 만약 tol=0.001이면, 손실함수의 변화가 0.001 이하가 되는 시점에 학습이 종료된다.

마지막으로 SGDClassifier의 기본 loss는 hinge이다. 이는 SVM에서 쓰이는 손실 함수인데, 알아만 두도록 하자. 